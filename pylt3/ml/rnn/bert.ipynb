{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Extract pre-computed feature vectors from a PyTorch BERT model.\"\"\"\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s', \n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The convention in BERT is:\n",
    "# (a) For sequence pairs:\n",
    "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "#  type_ids:   0   0  0    0    0     0      0   0    1  1  1   1  1   1\n",
    "# (b) For single sequences:\n",
    "#  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "#  type_ids:   0   0   0   0  0     0   0\n",
    "#\n",
    "# Where \"type_ids\" are used to indicate whether this is the first\n",
    "# sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "# `type=1` were learned during pre-training and are added to the wordpiece\n",
    "# embedding vector (and position vector). This is not *strictly* necessary\n",
    "# since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "# it easier for the model to learn the concept of sequences.\n",
    "#\n",
    "# For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "# used as as the \"sentence vector\". Note that this only makes sense because\n",
    "# the entire model is fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bert(sentences, max_seq_length, tokenizer):    \n",
    "    all_input_ids = []\n",
    "    all_input_mask = []\n",
    "    for sentence in sentences:\n",
    "        # tokenizer will also separate on punctuation\n",
    "        # see https://github.com/google-research/bert#tokenization\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        \n",
    "        # limit size of tokens\n",
    "        if len(tokens) > max_seq_length - 2:\n",
    "            tokens = tokens[0:(max_seq_length - 2)]\n",
    "        \n",
    "        # add [CLS] and [SEP], as expected in BERT\n",
    "        tokens = ['[CLS]', *tokens, '[SEP]']\n",
    "        \n",
    "        input_type_ids = [0] * len(tokens)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            input_mask.append(0)\n",
    "            \n",
    "        all_input_ids.append(input_ids)\n",
    "        all_input_mask.append(input_mask)\n",
    "        \n",
    "    return all_input_ids, all_input_mask\n",
    "\n",
    "\n",
    "def main(sentences, layers='-1, -2, -3, -4', max_seq_length=512, bert_model='bert-large-uncased', \n",
    "         do_lower_case=True, batch_size=32, no_cuda=False):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and not no_cuda else 'cpu')\n",
    "    \n",
    "    # 'layers' indicates which layers we want to concatenate\n",
    "    layer_idxs = [int(l) for l in layers.split(',')]\n",
    "    \n",
    "    # init tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)\n",
    "    \n",
    "    # returns a list of 'InputFeatures'\n",
    "    bert_features = prepare_bert(sentences, max_seq_length, tokenizer)\n",
    "    all_input_ids, all_input_mask = (torch.tensor(feat, dtype=torch.long) for feat in bert_features)\n",
    "    \n",
    "    # init model and move to device\n",
    "    model = BertModel.from_pretrained(bert_model)\n",
    "    model.to(device)    \n",
    "    \n",
    "    # prepare dataset and dataloader\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask)\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=batch_size)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for input_ids, input_mask in eval_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        all_encoder_layers, _ = model(input_ids, token_type_ids=None, attention_mask=input_mask)\n",
    "\n",
    "        layers_to_concat = [all_encoder_layers[idx] for idx in layer_idxs]\n",
    "        \n",
    "        concat = torch.cat(layers_to_concat, dim=-1)\n",
    "                \n",
    "        logger.info(concat.size())\n",
    "        logger.info(concat)\n",
    "        \n",
    "        # Pooling by also setting masked sequence items to zero\n",
    "        # Concat shape is [3, 32, 4096]\n",
    "        # Add 3rd dimensions to mask so that it is [3, 32, 1]\n",
    "        input_mask = input_mask.to(torch.float).unsqueeze(2)\n",
    "        # Multiply output with mask \n",
    "        pooled = concat * input_mask\n",
    "        # Sum items in sequence to get sentence representation\n",
    "        summed = torch.sum(pooled, dim=1).squeeze()\n",
    "        # Average over seq_length\n",
    "        divved = torch.div(summed, max_seq_length)\n",
    "        \n",
    "        logger.info(divved.size())\n",
    "        logger.info(divved)\n",
    "        \n",
    "        # OR (but don't do this, as it will ignore masks)\n",
    "        # Pooling by simple average pool\n",
    "        concat_permute = concat.permute(0, 2, 1)\n",
    "        \n",
    "        pooled = F.avg_pool1d(concat_permute, kernel_size=concat_permute.size(2)).squeeze()\n",
    "        \n",
    "        logger.info(pooled.size())\n",
    "        logger.info(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/26/2019 15:44:37 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at C:\\Users\\Bram\\.pytorch_pretrained_bert\\9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "03/26/2019 15:44:38 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz from cache at C:\\Users\\Bram\\.pytorch_pretrained_bert\\214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05\n",
      "03/26/2019 15:44:38 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file C:\\Users\\Bram\\.pytorch_pretrained_bert\\214d4777e8e3eb234563136cd3a49f6bc34131de836848454373fa43f10adc5e.abfbb80ee795a608acbf35c7bf2d2d58574df3887cdd94b355fc67e03fddba05 to temp dir C:\\Users\\Bram\\AppData\\Local\\Temp\\tmpv1ea8rsw\n",
      "03/26/2019 15:44:47 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "03/26/2019 15:44:57 - INFO - __main__ -   torch.Size([3, 32, 4096])\n",
      "03/26/2019 15:44:57 - INFO - __main__ -   tensor([[[-0.6289,  0.2882, -1.1056,  ...,  0.0429,  0.7017,  0.8096],\n",
      "         [-0.6917,  0.2038, -0.6520,  ..., -0.6563,  0.9098,  0.1183],\n",
      "         [-0.8669, -0.0830, -0.2590,  ..., -1.0497,  0.2540, -0.1612],\n",
      "         ...,\n",
      "         [-0.7817,  0.5367, -0.2018,  ...,  0.6944,  0.3518,  0.1718],\n",
      "         [ 0.0237,  0.1726, -0.2498,  ...,  0.3672,  0.8662,  1.4397],\n",
      "         [ 0.0032,  0.1290,  0.0249,  ...,  0.0651,  0.9898,  1.4952]],\n",
      "\n",
      "        [[-0.5957, -0.6583, -0.7550,  ..., -0.1543, -0.9311,  0.1252],\n",
      "         [-0.0383, -0.1341, -0.0448,  ...,  0.0242, -0.2916,  0.0202],\n",
      "         [ 0.1397, -0.4354, -0.4555,  ...,  0.2842,  0.8341, -0.2530],\n",
      "         ...,\n",
      "         [ 0.0621, -0.6834,  0.0853,  ...,  0.5941, -1.4050,  0.4390],\n",
      "         [ 0.0815, -0.0295,  0.2287,  ...,  0.6396,  0.6140,  0.2159],\n",
      "         [ 0.1222,  0.2789,  0.2101,  ...,  1.4225,  0.1964, -0.1205]],\n",
      "\n",
      "        [[ 0.0425, -0.5931, -1.0186,  ..., -0.0740, -0.0562, -1.4006],\n",
      "         [-0.2207, -0.2189, -0.5535,  ...,  0.0789, -0.6853,  0.0917],\n",
      "         [ 0.2322, -0.6639, -0.7299,  ..., -0.0613, -1.1489,  0.3656],\n",
      "         ...,\n",
      "         [ 0.8302, -0.5225, -0.0092,  ..., -0.7828, -0.0554, -0.4875],\n",
      "         [ 0.6038, -1.0042,  0.1465,  ..., -0.7974,  0.2381, -0.9419],\n",
      "         [ 0.5038, -0.6391,  0.3289,  ..., -0.6378,  1.1585, -0.5596]]],\n",
      "       device='cuda:0', grad_fn=<CatBackward>)\n",
      "03/26/2019 15:44:57 - INFO - __main__ -   torch.Size([3, 4096])\n",
      "03/26/2019 15:44:57 - INFO - __main__ -   tensor([[-0.1156,  0.0097, -0.1246,  ..., -0.0552,  0.0718,  0.0543],\n",
      "        [-0.0379, -0.0384, -0.0937,  ...,  0.0493, -0.0335,  0.0059],\n",
      "        [-0.0969, -0.1883, -0.2237,  ..., -0.0650, -0.0098, -0.1850]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "03/26/2019 15:44:57 - INFO - __main__ -   torch.Size([3, 4096])\n",
      "03/26/2019 15:44:57 - INFO - __main__ -   tensor([[-0.1273,  0.2798, -0.7181,  ...,  0.2939,  0.2164,  0.1590],\n",
      "        [ 0.0988,  0.0317, -0.3790,  ...,  0.5948, -0.8420, -0.3404],\n",
      "        [ 0.0218, -0.3431, -0.4059,  ..., -0.0505, -0.1551, -0.5678]],\n",
      "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    proc_args = {\n",
    "        'sentences': ['I saw Bert today !', 'Do you like bananas ?', 'Some sentences are really horrendous to parse .'],\n",
    "        'max_seq_length': 32\n",
    "    }\n",
    "    main(**proc_args)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        \n",
    "        concat = torch.cat(layers_to_concat, dim=-1)\n",
    "        concat = concat.permute(0, 2, 1)\n",
    "        logger.info(concat.size())\n",
    "#         logger.info(concat)\n",
    "        \n",
    "        pooled = F.avg_pool1d(concat, kernel_size=concat.size(2)).squeeze()\n",
    "        \n",
    "        logger.info(pooled.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
