{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-04-03 15:24:30,619: 'pattern' package found; tag filters are available for English\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'C:\\Python\\projects\\PyLT3')\n",
    "\n",
    "from pylt3.ml.rnn.LazyTextDataset import LazyTextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make results reproducible\n",
    "torch.manual_seed(3)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "np.random.seed(3)\n",
    "\n",
    "# run all numpy warnings as errors:\n",
    "np.seterr(all='raise')\n",
    "\n",
    "logging.basicConfig(format='[%(levelno)s] %(asctime)s: %(message)s', datefmt='%d-%b-%y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec and/or ELMo and/or BERT initialisation\n",
    "use_ms = True\n",
    "\n",
    "use_google_w2v = False\n",
    "use_custom_w2v = False\n",
    "use_elmo = False\n",
    "use_bert = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-04-03 15:24:30,739: Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n",
      "[INFO] 2019-04-03 15:24:31,308: loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\Bram\\.pytorch_pretrained_bert\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "if use_ms:\n",
    "    from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "if len(list(filter(None, (use_google_w2v, use_custom_w2v)))) > 1:\n",
    "    raise ValueError(f\"Only one of 'use_google_w2v' or 'use_custom_w2v' can be used.\")\n",
    "else:\n",
    "    use_w2v = any((use_google_w2v, use_custom_w2v))\n",
    "    if use_w2v:\n",
    "        import gensim\n",
    "        if use_google_w2v:\n",
    "            embed_p = Path(r'C:\\Users\\Bram\\Downloads\\GoogleNews-vectors-negative300.bin').resolve()\n",
    "            w2v_model = gensim.models.KeyedVectors.load_word2vec_format(str(embed_p), binary=True)\n",
    "            # add a padding token with only zeros\n",
    "            # TODO: find alternative for unknown tokens. Zeros is NOT a good idea\n",
    "            w2v_model.add(['@pad@', '@unk@'], [np.zeros(w2v_model.vectors.shape[1]), np.zeros(w2v_model.vectors.shape[1])])\n",
    "        elif use_custom_w2v:\n",
    "            embed_p = Path('..\\..\\..\\data\\dpc\\ml\\other\\dpc+news2017.dim146-ep10-min2-win10-repl.w2v_model').resolve()\n",
    "            w2v_model = gensim.models.KeyedVectors.load_word2vec_format(str(embed_p))\n",
    "            # add a padding token with only zeros\n",
    "            w2v_model.add(['@pad@'], [np.zeros(w2v_model.vectors.shape[1])])\n",
    "            \n",
    "        w2v_config = {\n",
    "            'model': w2v_model,\n",
    "            'weights': torch.FloatTensor(w2v_model.vectors)\n",
    "        }\n",
    "    else:\n",
    "        w2v_config = None\n",
    "        \n",
    "if use_elmo:\n",
    "    from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "    elmo_config = {\n",
    "        'options_url': 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json',\n",
    "        'weights_url': 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n",
    "    }\n",
    "else:\n",
    "    elmo_config = None\n",
    "    \n",
    "if use_bert:\n",
    "    from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "    from pytorch_pretrained_bert.modeling import BertModel\n",
    "    from pytorch_pretrained_bert.optimization import BertAdam\n",
    "    # For OOM errors, see https://github.com/google-research/bert/blob/master/README.md#out-of-memory-issues\n",
    "    # Comes down to: lower max_seq_len and batch_size\n",
    "    bert_config = {\n",
    "        'tokenizer': BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True),\n",
    "        'model_str': 'bert-base-uncased',\n",
    "        'layers': '-1,-2,-3,-4',\n",
    "        'max_seq_len': 64\n",
    "    }\n",
    "else:\n",
    "    bert_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self, hidden_dim=512, ms_dim=None, w2v=None, elmo=None, bert=None, bidirectional=True, drop_prob=0):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.ms_dim = ms_dim\n",
    "        self.w2v = w2v\n",
    "        self.elmo = elmo\n",
    "        self.bert = bert\n",
    "        self.bidirectional = bidirectional\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        input_fc = 0\n",
    "        \n",
    "        if ms_dim is not None:\n",
    "            self.ms_rnode = nn.GRU(ms_dim, hidden_dim, bidirectional=bidirectional, batch_first=True)\n",
    "            input_fc += hidden_dim\n",
    "            logging.info('Morphosyntactic features enabled...')\n",
    "            \n",
    "        if w2v is not None:\n",
    "            self.w2v = nn.Embedding.from_pretrained(w2v['weights'], freeze=True)        \n",
    "            self.w2v_rnode = nn.GRU(w2v['weights'].size(1), hidden_dim, bidirectional=bidirectional, batch_first=True)\n",
    "            input_fc += hidden_dim\n",
    "            logging.info('Word embeddings enabled...')\n",
    "        \n",
    "        if elmo is not None:\n",
    "            self.elmo = Elmo(elmo['options_url'], elmo['weights_url'], 1, dropout=drop_prob)\n",
    "#             self.elmo_rnode = nn.GRU(1024, hidden_dim, bidirectional=bidirectional)\n",
    "#             input_fc += hidden_dim\n",
    "            input_fc += 1024\n",
    "            logging.info('ELMo enabled...')\n",
    "            \n",
    "        if bert is not None:\n",
    "            self.bert = BertModel.from_pretrained(bert['model_str'])\n",
    "            # Freeze embeddings\n",
    "            for name, param in self.bert.named_parameters():                \n",
    "                if name.startswith('embeddings'):\n",
    "                    param.requires_grad = False\n",
    "            \n",
    "            self.bert_max_seq_len = bert['max_seq_len']\n",
    "            self.bert_layers = [int(l) for l in bert['layers'].split(',')]\n",
    "#             input_fc += 768 * len(self.bert_layers)\n",
    "            self.linear_bert = nn.Linear(768 * len(self.bert_layers), hidden_dim)\n",
    "            input_fc += hidden_dim\n",
    "            logging.info('Bert enabled...')\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob) if drop_prob > 0 else None\n",
    "        self.linear = nn.Linear(input_fc, 1)\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        \n",
    "    def _last_output(self, output, bidirectional=None):\n",
    "        bidirectional = self.bidirectional if bidirectional is None else bidirectional\n",
    "        if bidirectional:\n",
    "            sum_bi = output[:, :, :self.hidden_dim] + output[:, :, self.hidden_dim:]\n",
    "            last = sum_bi[:, -1, :]\n",
    "        else:\n",
    "            last = output[:, -1, :]\n",
    "    \n",
    "        return last    \n",
    "    \n",
    "    def forward(self, batch_size, ms_input=None, w2v_ids=None, elmo_ids=None, bert_input=None):            \n",
    "        if self.ms_dim is not None and ms_input is not None:\n",
    "            packed_ms_out, _ = self.ms_rnode(ms_input)\n",
    "#             print('packed ms output', packed_ms_out.data.size())            \n",
    "            # unpacked\n",
    "            unpacked_ms_out, ms_lengths = pad_packed_sequence(packed_ms_out, batch_first=True)\n",
    "#             print('unpacked ms input', unpacked_ms_out.size())\n",
    "            \n",
    "            if self.bidirectional:\n",
    "                # Sum two bidirectional layers\n",
    "                unpacked_ms_out = unpacked_ms_out[:, :, :self.hidden_dim] + unpacked_ms_out[:, :, self.hidden_dim:]\n",
    "#                 print('summed ms input', unpacked_ms_out.size())\n",
    "            # Get last item of each sequence, based on their *actual* lengths\n",
    "            final_ms = unpacked_ms_out[torch.arange(len(ms_lengths)), ms_lengths-1, :]\n",
    "#             print('final ms input', final_ms.size())\n",
    "        else:\n",
    "            final_ms = None\n",
    "        \n",
    "        if self.w2v is not None and w2v_ids is not None:            \n",
    "#             print('w2v_ids', w2v_ids.size())\n",
    "            w2v_out = self.w2v(w2v_ids)            \n",
    "#             print('w2v_out', w2v_out.size())\n",
    "            w2v_out, _ = self.w2v_rnode(w2v_out)\n",
    "            final_w2v = self._last_output(w2v_out)\n",
    "#             print('last w2v', final_w2v.size())\n",
    "        else:\n",
    "            final_w2v = None         \n",
    "                \n",
    "        if self.elmo is not None and elmo_ids is not None:\n",
    "            elmo_out = self.elmo(elmo_ids)\n",
    "            elmo_out = elmo_out['elmo_representations'][0]\n",
    "#             print('elmo representation size', elmo_out.size())\n",
    "#             elmo_out, _ = self.elmo_rnode(elmo_out)\n",
    "            \n",
    "#             final_elmo = self._last_output(elmo_out)\n",
    "            final_elmo = elmo_out[:, -1, :]\n",
    "#             print('last elmo', final_elmo.size())\n",
    "        else:\n",
    "            final_elmo = None\n",
    "        \n",
    "        if self.bert is not None and bert_input is not None:\n",
    "            bert_ids, bert_mask = bert_input\n",
    "              \n",
    "            all_bert_layers, _ = self.bert(bert_ids, attention_mask=bert_mask)\n",
    "            bert_concat = torch.cat([all_bert_layers[i] for i in self.bert_layers], dim=-1)\n",
    "            # Pooling by also setting masked items to zero\n",
    "            bert_mask = bert_mask.to(torch.float).unsqueeze(2)\n",
    "            # Multiply output with mask \n",
    "            bert_pooled = bert_concat * bert_mask\n",
    "            \n",
    "#             print('bert pooled', bert_pooled.size())\n",
    "            \n",
    "            # First item ['CLS'] should be sentence representation\n",
    "            final_bert = bert_pooled[:, 0, :]\n",
    "            final_bert = self.linear_bert(final_bert)\n",
    "            \n",
    "            # FOR AVERAGING instead of taking CLS node\n",
    "            \"\"\"\n",
    "            # Sum items in sequence to get sentence representation\n",
    "            bert_summed = torch.sum(bert_pooled, dim=1).squeeze()\n",
    "            # Average over seq_length\n",
    "            final_bert = torch.div(bert_summed, self.bert_max_seq_len)\n",
    "            \"\"\"          \n",
    "            \n",
    "#             print('final bert', final_bert.size())\n",
    "        else:\n",
    "            final_bert = None  \n",
    "        \n",
    "        sentence_finals = [final for final in [final_w2v, final_elmo, final_bert] if final is not None]\n",
    "        \n",
    "        # Sentence features concatenate\n",
    "        if len(sentence_finals) > 1:\n",
    "            sentence_cat = torch.cat(sentence_finals, dim=1)\n",
    "        elif len(sentence_finals) == 1:\n",
    "            sentence_cat = sentence_finals[0]\n",
    "        else:\n",
    "            sentence_cat = None\n",
    "        \n",
    "        if sentence_cat is not None:\n",
    "#             print('sentence_cat', sentence_cat.size())\n",
    "              pass\n",
    "\n",
    "        # Concatenating    \n",
    "        if final_ms is not None and sentence_cat is not None:\n",
    "            sentence_ms_cat = torch.cat((sentence_cat, final_ms), dim=1)\n",
    "        elif final_ms is not None:\n",
    "            sentence_ms_cat = final_ms\n",
    "        elif sentence_cat is not None:\n",
    "            sentence_ms_cat = sentence_cat\n",
    "        \n",
    "#         print('sentence_ms_cat', sentence_ms_cat.size())\n",
    "            \n",
    "        # Only use the last item's output\n",
    "        if self.drop_prob > 0:\n",
    "            sentence_ms_cat = self.dropout(sentence_ms_cat)\n",
    "        \n",
    "        regression = self.linear(sentence_ms_cat)\n",
    "        regression = self.lrelu(regression)\n",
    "        \n",
    "#         print('regression', regression.size())\n",
    "\n",
    "        return regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionRNN:\n",
    "    def __init__(self, train_files=None, valid_files=None, test_files=None, ms_dim=None, use_w2v=False,\n",
    "                 use_elmo=False, bert=None, batch_size=(64, 64)):\n",
    "        print('Using torch ' + torch.__version__)\n",
    "\n",
    "        self.datasets, self.dataloaders = RegressionRNN._set_data_loaders(train_files, valid_files, test_files, batch_size)\n",
    "        self.device = RegressionRNN._set_device()\n",
    "        \n",
    "        self.use_ms = True if ms_dim is not None else False\n",
    "        self.ms_dim = ms_dim\n",
    "        self.use_w2v = use_w2v\n",
    "        self.use_elmo = use_elmo\n",
    "        \n",
    "        self.use_bert = True if bert is not None else False\n",
    "        self.bert_tokenizer = bert_config['tokenizer'] if bert is not None else None\n",
    "        self.bert_max_seq_len = bert_config['max_seq_len'] if bert else None\n",
    "        \n",
    "        self.use_sentences = any((use_w2v, use_elmo, self.use_bert))\n",
    "        \n",
    "        if len(list(filter(None, [self.use_ms, self.use_sentences]))) + 1 != len(train_files):\n",
    "            logging.warning('The number of input files is not the same as the number of enabled features.'\n",
    "                            ' Be warned!')\n",
    "        \n",
    "        self.model = None\n",
    "        self.w2v_vocab = None\n",
    "        self.criterion = None\n",
    "        self.optimizer = None\n",
    "        self.bert_optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.checkpoint_f = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _set_data_loaders(train_files, valid_files, test_files, batch_size):\n",
    "        RegressionRNN._verify_input(train_files, valid_files, test_files)\n",
    "\n",
    "        datasets = {\n",
    "            'train': LazyTextDataset(train_files) if train_files is not None else None,\n",
    "            'valid': LazyTextDataset(valid_files) if valid_files is not None else None,\n",
    "            'test': LazyTextDataset(test_files) if test_files is not None else None\n",
    "        }\n",
    "        \n",
    "        logging.info(f\"Training set size: {len(datasets['train'])}\")\n",
    "        \n",
    "        if valid_files:\n",
    "            logging.info(f\"Validation set size: {len(datasets['valid'])}\")\n",
    "        if test_files:\n",
    "            logging.info(f\"Test set size: {len(datasets['test'])}\")                     \n",
    "\n",
    "        dataloaders = {\n",
    "            'train': DataLoader(datasets['train'], batch_size=batch_size[0], shuffle=True, num_workers=6) if train_files is not None else None,\n",
    "            'valid': DataLoader(datasets['valid'], batch_size=batch_size[1], shuffle=True, num_workers=6) if valid_files is not None else None,\n",
    "            'test': DataLoader(datasets['test'], batch_size=batch_size[1], shuffle=True, num_workers=6) if test_files is not None else None\n",
    "        }\n",
    "\n",
    "        return datasets, dataloaders\n",
    "\n",
    "    @staticmethod\n",
    "    def _set_device():\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            device_id = torch.cuda.current_device()\n",
    "            logging.info(f\"Using GPU {torch.cuda.get_device_name(device_id)}\")\n",
    "            # logging.info('Memory Usage:')\n",
    "            # logging.info('Allocated:', round(torch.cuda.memory_allocated(device_id) / 1024 ** 3, 1), 'GB')\n",
    "            # logging.info('Cached:   ', round(torch.cuda.memory_cached(device_id) / 1024 ** 3, 1), 'GB')\n",
    "        else:\n",
    "            logging.info('Using CPU...')\n",
    "\n",
    "        return device\n",
    "\n",
    "    @staticmethod\n",
    "    def _verify_input(train_files, test_files, valid_files):\n",
    "        for f_kind in [train_files, test_files, valid_files]:\n",
    "            if f_kind is None:\n",
    "                continue\n",
    "\n",
    "            for f in f_kind:\n",
    "                if not Path(f).resolve().is_file():\n",
    "                    raise ValueError(f\"Input file {str(f)} does not exist.\")  \n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_lines(data, split_on=None, cast_to=None):\n",
    "        out = []\n",
    "        for line in data:\n",
    "            line = line.strip()\n",
    "            if split_on:\n",
    "                line = line.split(split_on)\n",
    "                line = list(filter(None, line))\n",
    "            else:\n",
    "                line = [line] \n",
    "\n",
    "            if cast_to is not None:\n",
    "                line = [cast_to(l) for l in line]\n",
    "                \n",
    "            out.append(line)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def prepare_ms(self, data):\n",
    "        def pad_array(data, max_size):\n",
    "            \"\"\" Pad all 'sentences' to the size of the largest sentence. \"\"\"\n",
    "            # number of features in deepest list\n",
    "            feature_size = len(data[0][0])\n",
    "\n",
    "            for idx, arr in enumerate(data):\n",
    "                # If the list is smaller than the largest list -> pad it\n",
    "                if len(arr) < max_size:\n",
    "                    # Create a new zero-only array\n",
    "                    pad = np.zeros((max_size, feature_size), dtype=np.int8)\n",
    "                    # Replace the first part with the actual list\n",
    "                    pad[:len(arr)] = arr\n",
    "                    # Replace in-place in the top-level list\n",
    "                    data[idx] = pad\n",
    "\n",
    "            return np.array(data).reshape(-1, max_size, feature_size)\n",
    "\n",
    "        seqs = []\n",
    "        # Data is a 'text' of 'sentences'\n",
    "        for line in data:\n",
    "            # Every line is a 'sentence' of tab-separated 'tokens' \n",
    "            line = line.strip()\n",
    "            # Every token is a 'word' which is a list of 0s and 1s\n",
    "            tokens = line.split('\\t')\n",
    "            tokens = [list((map(int, token.split(' ')))) for token in tokens]            \n",
    "            seqs.append(np.array(tokens))      \n",
    "                         \n",
    "        \n",
    "        # Get the length of the not-padded sequences\n",
    "        lengths = torch.LongTensor([len(s) for s in seqs])\n",
    "        \n",
    "        # Create zero-only dataset                 \n",
    "        seq_tensor = torch.zeros(len(seqs), lengths.max(), self.ms_dim)\n",
    "        \n",
    "        # Fill in real values                 \n",
    "        for idx, (seq, seqlen) in enumerate(zip(seqs, lengths)):\n",
    "            seq_tensor[idx, :seqlen] = torch.FloatTensor(seq)              \n",
    "\n",
    "        # Gets back sorted lengths and the indices of sorted items\n",
    "        lengths, sorted_idxs = torch.sort(lengths, dim=0, descending=True)\n",
    "        # Sort tensor by using sorted indices\n",
    "#         print(sorted_idxs)\n",
    "        seq_tensor = seq_tensor[sorted_idxs, :, :]\n",
    "        # Pack sequences\n",
    "        packed_seqs = pack_padded_sequence(seq_tensor, lengths, batch_first=True)\n",
    "        \n",
    "        return packed_seqs, sorted_idxs\n",
    "                         \n",
    "    def prepare_w2v(self, data):\n",
    "        \"\"\" Gets the word2vec ID of the tokens.\n",
    "            Input is a batch of sentences, consisting of tokens. \"\"\"\n",
    "        idxs = []\n",
    "        # Get size of longest sequence\n",
    "        max_length = max([len(seq) for seq in data])\n",
    "\n",
    "        for seq in data:\n",
    "            tok_idxs = []\n",
    "            for word in seq:\n",
    "                try:\n",
    "                    tok_idxs.append(self.w2v_vocab[word].index)\n",
    "                except KeyError:\n",
    "                    tok_idxs.append(self.w2v_vocab['@unk@'].index)\n",
    "            \n",
    "            # Pad current sequence if smaller than largest sequence\n",
    "            seq_length = len(seq)\n",
    "            if seq_length < max_length:\n",
    "                tok_idxs.extend([self.w2v_vocab['@pad@'].index] * (max_length - seq_length))\n",
    "\n",
    "            idxs.append(tok_idxs)\n",
    "        \n",
    "        idxs = torch.LongTensor(idxs)\n",
    "        return idxs   \n",
    "                                 \n",
    "    @staticmethod\n",
    "    def prepare_elmo(sentences):\n",
    "        # Add <S> and </S> tokens to sentence\n",
    "        # See https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md#notes-on-statefulness-and-non-determinism        \n",
    "        elmo_sentences = []\n",
    "        for s in sentences:\n",
    "            elmo_sentences.append(['<S>', *s, '</S>'])\n",
    "        \n",
    "        return elmo_sentences\n",
    "    \n",
    "    def prepare_bert(self, sentences):\n",
    "        all_input_ids = []\n",
    "        all_input_mask = []\n",
    "        for sentence in sentences:\n",
    "            sentence = ' '.join(sentence)\n",
    "            # tokenizer will also separate on punctuation\n",
    "            # see https://github.com/google-research/bert#tokenization\n",
    "            tokens = self.bert_tokenizer.tokenize(sentence)\n",
    "\n",
    "            # limit size of tokens\n",
    "            if len(tokens) > self.bert_max_seq_len - 2:\n",
    "                tokens = tokens[0:(self.bert_max_seq_len - 2)]\n",
    "\n",
    "            # add [CLS] and [SEP], as expected in BERT\n",
    "            tokens = ['[CLS]', *tokens, '[SEP]']\n",
    "\n",
    "            input_type_ids = [0] * len(tokens)\n",
    "            input_ids = self.bert_tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "            # tokens are attended to.\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.\n",
    "            while len(input_ids) < self.bert_max_seq_len:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "\n",
    "            all_input_ids.append(input_ids)\n",
    "            all_input_mask.append(input_mask)\n",
    "        \n",
    "        all_input_ids = torch.LongTensor(all_input_ids)\n",
    "        all_input_mask = torch.LongTensor(all_input_mask)\n",
    "                         \n",
    "        return all_input_ids, all_input_mask                             \n",
    "                         \n",
    "    @staticmethod\n",
    "    def _plot_training(train_losses, valid_losses):\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_losses, label='Training loss')\n",
    "        plt.plot(valid_losses, label='Validation loss')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.legend(frameon=False)\n",
    "        plt.title('Loss progress')\n",
    "\n",
    "        plt.show()\n",
    "        plt.savefig('progress.png')\n",
    "                \n",
    "    def train(self, epochs=10, checkpoint_f='checkpoint.pth', log_update_freq=0, patience=None):\n",
    "        logging.info('Training started.')        \n",
    "        train_start = time.time()\n",
    "        \n",
    "        self.checkpoint_f = checkpoint_f\n",
    "\n",
    "        valid_loss_min = np.inf\n",
    "        train_losses, valid_losses = [], []\n",
    "        last_saved_epoch = 0\n",
    "        # keep\n",
    "        total_train_time = 0\n",
    "        for epoch in range(epochs):\n",
    "            epoch_start = time.time()\n",
    "            \n",
    "            train_loss, train_results = self._process('train', log_update_freq, epoch)\n",
    "            total_train_time += time.time() - epoch_start\n",
    "            \n",
    "            valid_loss, valid_results = self._process('valid', log_update_freq, epoch)\n",
    "            \n",
    "            try:\n",
    "                train_pearson = pearsonr(train_results['predictions'], train_results['targets'])\n",
    "            except FloatingPointError:\n",
    "                train_pearson = \"Could not calculate Pearsonr\"\n",
    "            \n",
    "            try:\n",
    "                valid_pearson = pearsonr(valid_results['predictions'], valid_results['targets'])\n",
    "            except FloatingPointError:\n",
    "                valid_pearson = \"Could not calculate Pearsonr\"\n",
    "            \n",
    "            # calculate average losses\n",
    "            train_loss = np.mean(train_loss)\n",
    "            valid_loss = np.mean(valid_loss)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "            \n",
    "            # print training/validation statistics            \n",
    "            logging.info(f\"Epoch {epoch} - completed in {(time.time() - epoch_start):.0f} seconds\\n\"\n",
    "                         f\"Training Loss: {train_loss:.6f}\\t Pearson: {train_pearson}\\n\"\n",
    "                         f\"Validation loss: {valid_loss:.6f}\\t Pearson: {valid_pearson}\")\n",
    "            \n",
    "            # save model if validation loss has decreased\n",
    "            if valid_loss <= valid_loss_min:\n",
    "                logging.info(f'!! Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}).')\n",
    "                logging.info(f'!! Saving model as {self.checkpoint_f}...')\n",
    "                \n",
    "                torch.save(self.model.state_dict(), self.checkpoint_f)\n",
    "                last_saved_epoch = epoch                         \n",
    "                valid_loss_min = valid_loss\n",
    "            else:\n",
    "                logging.info(f\"!! Valid loss not improved. (Min. = {valid_loss_min}; last save at ep. {last_saved_epoch})\")\n",
    "                if train_loss <= valid_loss:\n",
    "                    logging.warning(f\"!! Training loss is lte validation loss. Might be overfitting!\")\n",
    "            \n",
    "            # Early-stopping\n",
    "            if patience is not None:\n",
    "                if (epoch - last_saved_epoch) == patience:\n",
    "                    logging.info(f\"Stopping early at epoch {epoch} (patience={patience})...\")\n",
    "                    break\n",
    "            \n",
    "            # Optimise with scheduler\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step(valid_loss)\n",
    "            \n",
    "        RegressionRNN._plot_training(train_losses, valid_losses)\n",
    "        \n",
    "        logging.info(f\"Training completed in {(time.time() - train_start):.0f} seconds\"\n",
    "                     f\"\\nMin. valid loss: {valid_loss_min}\\nLast saved epoch: {last_saved_epoch}\"\n",
    "                     f\"\\nPerformance: {len(self.datasets['train'])//total_train_time:.0f} sentences/s\")\n",
    "\n",
    "    def _process(self, do, log_update_freq, epoch=None):\n",
    "        if do not in ('train', 'valid', 'test'):\n",
    "            raise ValueError(\"Use 'train', 'valid', or 'test' for 'do'.\")\n",
    "        \n",
    "        results = {'predictions': np.array([]), 'targets': np.array([])}\n",
    "        losses = np.array([])\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        if do == 'train':\n",
    "            self.model.train()            \n",
    "            torch.set_grad_enabled(True)\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            torch.set_grad_enabled(False)\n",
    "        \n",
    "        if log_update_freq:\n",
    "            nro_batches = len(self.datasets[do]) // self.dataloaders[do].batch_size\n",
    "            update_interval = nro_batches * (log_update_freq/100)\n",
    "            update_checkpoints = {int(nro_batches-(i*update_interval)) for i in range((100//log_update_freq))}\n",
    "        \n",
    "        for batch_idx, data in enumerate(self.dataloaders[do], 1):\n",
    "            # 0. Clear gradients\n",
    "            if do == 'train':                \n",
    "                self.optimizer.zero_grad()\n",
    "                if self.use_bert and self.bert_optimizer is not None:\n",
    "                    self.bert_optimizer.zero_grad()                     \n",
    "                \n",
    "            # 1. Data prep\n",
    "            if self.use_ms:\n",
    "                ms = data[0]\n",
    "                if self.use_sentences:\n",
    "                    sentence = data[1]\n",
    "            elif self.use_sentences:\n",
    "                sentence = data[0]\n",
    "                     \n",
    "            target = data[-1]\n",
    "                     \n",
    "            # Convert ms features to int array\n",
    "            if self.use_ms: \n",
    "                ms, sorted_ids = self.prepare_ms(ms)\n",
    "                ms = ms.to(self.device)\n",
    "            else:\n",
    "                ms = None\n",
    "            \n",
    "            # Convert sentence to token array\n",
    "            if self.use_sentences:\n",
    "                sentence = self.prepare_lines(sentence, split_on=' ')\n",
    "            \n",
    "            # Convert tokens to word2vec IDs\n",
    "            if self.use_w2v:\n",
    "                w2v_ids = self.prepare_w2v(sentence)\n",
    "                w2v_ids = w2v_ids[sorted_ids] if ms is not None else w2v_ids\n",
    "                w2v_ids = w2v_ids.to(self.device)\n",
    "            else:\n",
    "                w2v_ids = None   \n",
    "            \n",
    "            if self.use_elmo:\n",
    "                elmo_sentence = RegressionRNN.prepare_elmo(sentence)\n",
    "                elmo_ids = batch_to_ids(elmo_sentence)                     \n",
    "                elmo_ids = elmo_ids[sorted_ids] if ms is not None else elmo_ids\n",
    "                elmo_ids = elmo_ids.to(self.device)\n",
    "            else:\n",
    "                elmo_ids = None\n",
    "            \n",
    "            if self.use_bert:\n",
    "                bert_ids, bert_mask = self.prepare_bert(sentence)                     \n",
    "                bert_ids = bert_ids[sorted_ids] if ms is not None else bert_ids\n",
    "                bert_mask = bert_mask[sorted_ids] if ms is not None else bert_mask\n",
    "                     \n",
    "                bert_ids = bert_ids.to(self.device)\n",
    "                bert_mask = bert_mask.to(self.device)\n",
    "                bert_input = (bert_ids, bert_mask)\n",
    "            else:\n",
    "                bert_input = None\n",
    "            \n",
    "            # Convert target to float array\n",
    "            target = torch.Tensor(self.prepare_lines(target, cast_to=float))\n",
    "            target = target[sorted_ids] if ms is not None else target\n",
    "            # Get current batch size\n",
    "            curr_batch_size = target.size(0)           \n",
    "                     \n",
    "            target = target.to(self.device)   \n",
    "\n",
    "            # 2. Predictions\n",
    "            pred = self.model(curr_batch_size, ms, w2v_ids, elmo_ids, bert_input)\n",
    "            loss = self.criterion(pred, target)\n",
    "                    \n",
    "            # 3. Optimise during training\n",
    "            if do == 'train':\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if self.use_bert and self.bert_optimizer is not None:\n",
    "                    self.bert_optimizer.step()\n",
    "            \n",
    "            # 4. Save results\n",
    "            pred = pred.detach().cpu().numpy()\n",
    "            target = target.cpu().numpy()\n",
    "            \n",
    "            results['predictions'] = np.append(results['predictions'], pred, axis=None)            \n",
    "            results['targets'] = np.append(results['targets'], target, axis=None)\n",
    "            losses = np.append(losses, float(loss))\n",
    "\n",
    "            if log_update_freq and batch_idx in update_checkpoints:\n",
    "                if do in ('train', 'valid'):\n",
    "                    logging.info(f\"{do.capitalize()} epoch {epoch}, batch nr. {batch_idx}/{nro_batches}...\")\n",
    "                else:                        \n",
    "                    logging.info(f\"{do.capitalize()}, batch nr. {batch_idx}/{nro_batches}...\")\n",
    "        \n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        return losses, results\n",
    "\n",
    "    def test(self, checkpoint_f='checkpoint.pth', log_update_freq=0):\n",
    "        logging.info('Testing started.')\n",
    "        test_start = time.time()\n",
    "        \n",
    "        if self.checkpoint_f is None:\n",
    "            self.model.load_state_dict(torch.load(checkpoint_f, map_location=self.device))\n",
    "        else:\n",
    "            self.model.load_state_dict(torch.load(self.checkpoint_f, map_location=self.device))\n",
    "\n",
    "        test_loss, test_results = self._process('test', log_update_freq)\n",
    "\n",
    "        try:\n",
    "            test_pearson = pearsonr(test_results['predictions'], test_results['targets'])\n",
    "        except FloatingPointError:\n",
    "            test_pearson = \"Could not calculate Pearsonr\"\n",
    "        \n",
    "        test_loss = np.mean(test_loss)\n",
    "        \n",
    "        logging.info(f\"Testing completed in {(time.time() - test_start):.0f} seconds\"\n",
    "                     f\"\\nLoss: {test_loss:.6f}\\t Pearson: {test_pearson}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 1.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-04-03 15:24:31,448: Training set size: 113633\n",
      "[INFO] 2019-04-03 15:24:31,448: Validation set size: 8000\n",
      "[INFO] 2019-04-03 15:24:31,449: Test set size: 8000\n",
      "[INFO] 2019-04-03 15:24:31,623: Using GPU GeForce GTX 1080 Ti\n",
      "[INFO] 2019-04-03 15:24:31,627: Morphosyntactic features enabled...\n",
      "[INFO] 2019-04-03 15:24:32,079: loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at C:\\Users\\Bram\\.pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "[INFO] 2019-04-03 15:24:32,081: extracting archive file C:\\Users\\Bram\\.pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir C:\\Users\\Bram\\AppData\\Local\\Temp\\tmphs23oh2s\n",
      "[INFO] 2019-04-03 15:24:34,920: Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO] 2019-04-03 15:24:36,677: Bert enabled...\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 256\n",
    "MS_SIZE = 102 if use_ms else None\n",
    "\n",
    "files = {\n",
    "    'train_files': (r'C:\\wsl-shared\\cross-conll\\train\\conll-feats.train',\n",
    "                    r'C:\\wsl-shared\\cross-conll\\train\\dpc.tok.norm.clean.cut.en.train',                                  \n",
    "                    r'C:\\wsl-shared\\cross-conll\\train\\cross.txt'),\n",
    "     'valid_files': (r'C:\\wsl-shared\\cross-conll\\dev\\conll-feats.dev',\n",
    "                     r'C:\\wsl-shared\\cross-conll\\dev\\dpc.tok.norm.clean.cut.en.dev',\n",
    "                     r'C:\\wsl-shared\\cross-conll\\dev\\cross.txt'),\n",
    "     'test_files': (r'C:\\wsl-shared\\cross-conll\\test\\conll-feats.test',\n",
    "                    r'C:\\wsl-shared\\cross-conll\\test\\dpc.tok.norm.clean.cut.en.test',                                 \n",
    "                    r'C:\\wsl-shared\\cross-conll\\test\\cross.txt'),\n",
    "}\n",
    "\n",
    "regr = RegressionRNN(**files,\n",
    "                     ms_dim = MS_SIZE,\n",
    "                     use_w2v=use_w2v,\n",
    "                     use_elmo=use_elmo,\n",
    "                     bert=bert_config,\n",
    "                     batch_size=(64, 64))\n",
    "\n",
    "if use_w2v:\n",
    "    regr.w2v_vocab = w2v_config['model'].vocab        \n",
    "\n",
    "regr.model = Regressor(HIDDEN_DIM, \n",
    "                       ms_dim=MS_SIZE, \n",
    "                       w2v=w2v_config,\n",
    "                       elmo=elmo_config,\n",
    "                       bert=bert_config,\n",
    "                       bidirectional=True,\n",
    "                       drop_prob=0.5)\n",
    "\n",
    "regr.criterion = nn.MSELoss()\n",
    "regr.optimizer = BertAdam([p for p in regr.model.bert.parameters() if p.requires_grad],\n",
    "                          lr=0.00001,\n",
    "                          weight_decay= 0.002)\n",
    "\n",
    "# regr.optimizer = optim.Adam([\n",
    "#     {'params': [p for name, p in regr.model.named_parameters() if p.requires_grad and not name.startswith('bert')],\n",
    "#      'lr': 0.001},\n",
    "#     {'params': [p for p in regr.model.bert.parameters() if p.requires_grad],\n",
    "#      'lr': 0.00002,\n",
    "#      'weight_decay': 0.0002}\n",
    "# ])\n",
    "\n",
    "# regr.scheduler = optim.lr_scheduler.ReduceLROnPlateau(regr.optimizer, 'min', factor=0.1, patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-04-03 15:24:36,682: Training started.\n",
      "[INFO] 2019-04-03 15:28:50,819: Train epoch 0, batch nr. 443/1775...\n",
      "[INFO] 2019-04-03 15:32:59,142: Train epoch 0, batch nr. 887/1775...\n",
      "[INFO] 2019-04-03 15:37:10,455: Train epoch 0, batch nr. 1331/1775...\n",
      "[INFO] 2019-04-03 15:41:21,043: Train epoch 0, batch nr. 1775/1775...\n",
      "[INFO] 2019-04-03 15:41:30,461: Valid epoch 0, batch nr. 31/125...\n",
      "[INFO] 2019-04-03 15:41:35,569: Valid epoch 0, batch nr. 62/125...\n",
      "[INFO] 2019-04-03 15:41:40,723: Valid epoch 0, batch nr. 93/125...\n",
      "[INFO] 2019-04-03 15:41:46,164: Valid epoch 0, batch nr. 125/125...\n",
      "[INFO] 2019-04-03 15:41:46,674: Epoch 0 - completed in 1030 seconds\n",
      "Training Loss: 0.885929\t Pearson: (0.52563387291768, 0.0)\n",
      "Validation loss: 0.846196\t Pearson: (0.5953006890254983, 0.0)\n",
      "[INFO] 2019-04-03 15:41:46,674: !! Validation loss decreased (inf --> 0.846196).\n",
      "[INFO] 2019-04-03 15:41:46,675: !! Saving model as checkpoint.pth...\n",
      "[INFO] 2019-04-03 15:46:00,275: Train epoch 1, batch nr. 443/1775...\n",
      "[INFO] 2019-04-03 15:50:06,956: Train epoch 1, batch nr. 887/1775...\n",
      "[INFO] 2019-04-03 15:54:17,852: Train epoch 1, batch nr. 1331/1775...\n",
      "[INFO] 2019-04-03 15:58:26,400: Train epoch 1, batch nr. 1775/1775...\n",
      "[INFO] 2019-04-03 15:58:35,315: Valid epoch 1, batch nr. 31/125...\n",
      "[INFO] 2019-04-03 15:58:40,332: Valid epoch 1, batch nr. 62/125...\n",
      "[INFO] 2019-04-03 15:58:45,305: Valid epoch 1, batch nr. 93/125...\n",
      "[INFO] 2019-04-03 15:58:50,477: Valid epoch 1, batch nr. 125/125...\n",
      "[INFO] 2019-04-03 15:58:50,991: Epoch 1 - completed in 1024 seconds\n",
      "Training Loss: 0.727251\t Pearson: (0.636898472031406, 0.0)\n",
      "Validation loss: 0.814785\t Pearson: (0.6039744135305201, 0.0)\n",
      "[INFO] 2019-04-03 15:58:50,992: !! Validation loss decreased (0.846196 --> 0.814785).\n",
      "[INFO] 2019-04-03 15:58:50,992: !! Saving model as checkpoint.pth...\n",
      "[INFO] 2019-04-03 16:03:04,686: Train epoch 2, batch nr. 443/1775...\n",
      "[INFO] 2019-04-03 16:07:13,449: Train epoch 2, batch nr. 887/1775...\n",
      "[INFO] 2019-04-03 16:11:23,369: Train epoch 2, batch nr. 1331/1775...\n",
      "[INFO] 2019-04-03 16:15:34,207: Train epoch 2, batch nr. 1775/1775...\n",
      "[INFO] 2019-04-03 16:15:43,030: Valid epoch 2, batch nr. 31/125...\n",
      "[INFO] 2019-04-03 16:15:48,004: Valid epoch 2, batch nr. 62/125...\n",
      "[INFO] 2019-04-03 16:15:52,977: Valid epoch 2, batch nr. 93/125...\n",
      "[INFO] 2019-04-03 16:15:58,087: Valid epoch 2, batch nr. 125/125...\n",
      "[INFO] 2019-04-03 16:15:58,606: Epoch 2 - completed in 1027 seconds\n",
      "Training Loss: 0.611487\t Pearson: (0.7072386371128143, 0.0)\n",
      "Validation loss: 0.851764\t Pearson: (0.6000757008227714, 0.0)\n",
      "[INFO] 2019-04-03 16:15:58,606: !! Valid loss not improved. (Min. = 0.8147851883172988; last save at ep. 1)\n",
      "[WARNING] 2019-04-03 16:15:58,607: !! Training loss is lte validation loss. Might be overfitting!\n",
      "[INFO] 2019-04-03 16:20:08,254: Train epoch 3, batch nr. 443/1775...\n",
      "[INFO] 2019-04-03 16:24:20,212: Train epoch 3, batch nr. 887/1775...\n",
      "[INFO] 2019-04-03 16:28:26,658: Train epoch 3, batch nr. 1331/1775...\n",
      "[INFO] 2019-04-03 16:32:33,549: Train epoch 3, batch nr. 1775/1775...\n",
      "[INFO] 2019-04-03 16:32:42,545: Valid epoch 3, batch nr. 31/125...\n",
      "[INFO] 2019-04-03 16:32:47,566: Valid epoch 3, batch nr. 62/125...\n",
      "[INFO] 2019-04-03 16:32:52,551: Valid epoch 3, batch nr. 93/125...\n",
      "[INFO] 2019-04-03 16:32:57,714: Valid epoch 3, batch nr. 125/125...\n",
      "[INFO] 2019-04-03 16:32:58,227: Epoch 3 - completed in 1020 seconds\n",
      "Training Loss: 0.512383\t Pearson: (0.7624052720956945, 0.0)\n",
      "Validation loss: 0.863497\t Pearson: (0.5970584261432335, 0.0)\n",
      "[INFO] 2019-04-03 16:32:58,228: !! Valid loss not improved. (Min. = 0.8147851883172988; last save at ep. 1)\n",
      "[WARNING] 2019-04-03 16:32:58,228: !! Training loss is lte validation loss. Might be overfitting!\n",
      "[INFO] 2019-04-03 16:37:11,733: Train epoch 4, batch nr. 443/1775...\n",
      "[INFO] 2019-04-03 16:41:22,351: Train epoch 4, batch nr. 887/1775...\n",
      "[INFO] 2019-04-03 16:45:33,069: Train epoch 4, batch nr. 1331/1775...\n"
     ]
    }
   ],
   "source": [
    "regr.train(epochs=100, log_update_freq=25, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.test(log_update_freq=25)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Sentences + MS\n",
    "files = {\n",
    "    'train_files': (r'C:\\wsl-shared\\cross-conll\\train\\conll-feats.train',\n",
    "                    r'C:\\wsl-shared\\cross-conll\\train\\dpc.tok.norm.clean.cut.en.train',                                  \n",
    "                    r'C:\\wsl-shared\\cross-conll\\train\\cross.txt'),\n",
    "     'valid_files': (r'C:\\wsl-shared\\cross-conll\\dev\\conll-feats.dev',\n",
    "                     r'C:\\wsl-shared\\cross-conll\\dev\\dpc.tok.norm.clean.cut.en.dev',\n",
    "                     r'C:\\wsl-shared\\cross-conll\\dev\\cross.txt'),\n",
    "     'test_files': (r'C:\\wsl-shared\\cross-conll\\test\\conll-feats.test',\n",
    "                    r'C:\\wsl-shared\\cross-conll\\test\\dpc.tok.norm.clean.cut.en.test',                                 \n",
    "                    r'C:\\wsl-shared\\cross-conll\\test\\cross.txt'),\n",
    "}\n",
    "\n",
    "# MS\n",
    "files = {\n",
    "    'train_files': (r'C:\\wsl-shared\\cross-conll\\train\\conll-feats.train',                                 \n",
    "                    r'C:\\wsl-shared\\cross-conll\\train\\cross.txt'),\n",
    "     'valid_files': (r'C:\\wsl-shared\\cross-conll\\dev\\conll-feats.dev',\n",
    "                     r'C:\\wsl-shared\\cross-conll\\dev\\cross.txt'),\n",
    "     'test_files': (r'C:\\wsl-shared\\cross-conll\\test\\conll-feats.test',                               \n",
    "                    r'C:\\wsl-shared\\cross-conll\\test\\cross.txt'),\n",
    "}\n",
    "\n",
    "# Sentences\n",
    "files = {\n",
    "    'train_files': (r'C:\\wsl-shared\\cross-conll\\train\\dpc.tok.norm.clean.cut.en.train',                                  \n",
    "                    r'C:\\wsl-shared\\cross-conll\\train\\cross.txt'),\n",
    "     'valid_files': (r'C:\\wsl-shared\\cross-conll\\dev\\dpc.tok.norm.clean.cut.en.dev',\n",
    "                     r'C:\\wsl-shared\\cross-conll\\dev\\cross.txt'),\n",
    "     'test_files': (r'C:\\wsl-shared\\cross-conll\\test\\dpc.tok.norm.clean.cut.en.test',                                 \n",
    "                    r'C:\\wsl-shared\\cross-conll\\test\\cross.txt'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
